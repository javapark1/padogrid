#
# Master system properties set via the SPARK_MASTER_OPTS environment variable.
# PadoGrid sets these values to SPARK_MASTER_OPTS during startup.
#

#
# The maximum number of completed applications to display. Older applications will be 
# dropped from the UI to maintain this limit.
# Default: 200
#   Since: 0.8.0
#
#spark.deploy.retainedApplications=200

#
# The maximum number of completed drivers to display. Older drivers will be dropped from 
# the UI to maintain this limit.
# Default: 200
#   Since: 1.1.0
#spark.deploy.retainedDrivers=200

#
# Whether the standalone cluster manager should spread applications out across nodes or try to consolidate
# them onto as few nodes as possible. Spreading out is usually better for data locality in HDFS, but
# consolidating is more efficient for compute-intensive workloads.
# Default: true
#   Since: 0.6.1
#
#spark.deploy.spreadOut=true

#
# Default number of cores to give to applications in Spark's standalone mode if they don't set spark.cores.max.
# If not set, applications always get all available cores unless they configure spark.cores.max themselves.
# Set this lower on a shared cluster to prevent users from grabbing the whole cluster by default.
# Default: (infinite) 	
#   Since: 0.9.0
#
#spark.deploy.defaultCores

#
# Limit on the maximum number of back-to-back executor failures that can occur before the standalone
# cluster manager removes a faulty application. An application will never be removed if it has any
# running executors. If an application experiences more than spark.deploy.maxExecutorRetries failures
# in a row, no executors successfully start running in between those failures, and the application has
# no running executors then the standalone cluster manager will remove the application and mark it as
# failed. To disable this automatic removal, set spark.deploy.maxExecutorRetries to -1.
# Default: 10
#   Since: 1.6.3
#
#spark.deploy.maxExecutorRetries=10

#
# Number of seconds after which the standalone deploy master considers a worker lost if it receives no
# heartbeats.
# Default: 60
#   Since: 0.6.2
#
# spark.worker.timeout=60 	
 
#
# Amount of a particular resource to use on the worker.
# Default: (none) 	
#   Since: 3.0.0
#
#spark.worker.resource.{resourceName}.amount

#
# Path to resource discovery script, which is used to find a particular resource while worker starting
# up. And the output of the script should be formatted like the ResourceInformation class.
# Default: (none) 	
#   Since: 3.0.0
#
#spark.worker.resource.{resourceName}.discoveryScript

#
# Path to resources file which is used to find various resources while worker starting up. The content
# of resources file should be formatted like [{"id":{"componentName": "spark.worker","resourceName":"gpu"},
# "addresses":["0","1","2"]}]. If a particular resource is not found in the resources file, the discovery
# script would be used to find that resource. If the discovery script also does not find the resources,
# the worker will fail to start up.
# Default: (none)
# Since: 3.0.0
#
#spark.worker.resourcesFile
