#
# Worker system properties set via the SPARK_WORKER_OPTS environment variable.
# PadoGrid sets these values to SPARK_WORKER_OPTS during startup.
#

#
# Enable periodic cleanup of worker / application directories. Note that this only affects
# standalone mode, as YARN works differently. Only the directories of stopped applications
# are cleaned up. This should be enabled if spark.shuffle.service.db.enabled is "true"
# Default: false
#   Since: 1.0.0
#
#spark.worker.cleanup.enabled=false

#
# Controls the interval, in seconds, at which the worker cleans up old application work dirs on
# the local machine.
# Default: 1800 (30 minutes) 	
#   Since: 1.0.0
#
#spark.worker.cleanup.interval=1800

#
# The number of seconds to retain application work directories on each worker. This is a Time To Live and
# should depend on the amount of available disk space you have. Application logs and jars are downloaded
# to each application work dir. Over time, the work dirs can quickly fill up disk space, especially if
# you run jobs very frequently.
# Default: 604800 (7 days, 7 * 24 * 3600)
#   Since: 1.0.0
#spark.worker.cleanup.appDataTtl=604800

#
# Store External Shuffle service state on local disk so that when the external shuffle service is
# restarted, it will automatically reload info on current executors. This only affects standalone
# mode (yarn always has this behavior enabled). You should also enable spark.worker.cleanup.enabled,
# to ensure that the state eventually gets cleaned up. This config may be removed in the future.
# Default: true
#   Since: 3.0.0
#
#spark.shuffle.service.db.enabled=true

#
# Enable cleanup non-shuffle files(such as temp. shuffle blocks, cached RDD/broadcast blocks,
# spill files, etc) of worker directories following executor exits. Note that this doesn't overlap
# with `spark.worker.cleanup.enabled`, as this enables cleanup of non-shuffle files in local directories
# of a dead executor, while `spark.worker.cleanup.enabled` enables cleanup of all files/subdirectories of
# a stopped and timeout application. This only affects Standalone mode, support of other cluster managers
# can be added in the future.
# Default: true
#   Since: 2.4.0
#
#spark.storage.cleanupFilesAfterExecutorExit=true

#
# For compressed log files, the uncompressed file can only be computed by uncompressing the files.
# Spark caches the uncompressed file size of compressed log files. This property controls the cache size.
# Default: 100
#   Since: 2.0.2
#
#spark.worker.ui.compressedLogFileLengthCacheSize=100
